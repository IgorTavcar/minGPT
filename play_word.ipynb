{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cordless-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "matched-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varying-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historic-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Shakespeare texts from URL. There are original texts and translated to modern English\n",
    "text_modern = urllib.request.urlopen('https://raw.githubusercontent.com/emukans/shakespeare-texts/master/all_modern.txt').read().decode(\"utf-8\", \"ignore\")\n",
    "text_original = urllib.request.urlopen('https://raw.githubusercontent.com/emukans/shakespeare-texts/master/all_original.txt').read().decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "boxed-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = self.build_vocab(data)\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.vocab) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.vocab) }\n",
    "    \n",
    "    def sort_vocab(self, vocab):\n",
    "        \"\"\"\n",
    "        Vocab should have the followind order: hashtag, numbers, characters sorted by length.\n",
    "        Hashtags should go first, because they will be used as dividers on tokenization step.\n",
    "        Numbers should go before characters, because token ids are numbers. Otherwise token ids will be considered as usual numbers and replaced twice.\n",
    "        \"\"\"\n",
    "        sorted_vocab = sorted(vocab, key=lambda x: len(x), reverse=True)\n",
    "        tag = [int(s) for s in sorted_vocab if s == '#']\n",
    "        \n",
    "        numeric = [int(s) for s in sorted_vocab if s.isnumeric()]\n",
    "        numeric = [str(s) for s in sorted(numeric, reverse=True)]\n",
    "        rest = [s for s in sorted_vocab if not s.isnumeric()]\n",
    "        \n",
    "        sorted_vocab = tag + numeric + rest\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        \"\"\"\n",
    "        Build vocabluary using BPE alghorithm.\n",
    "        \"\"\"\n",
    "        vocab = set(data)\n",
    "        if len(vocab) > self.vocab_size:\n",
    "            raise ValueError('Vocab size should be greater than unique char count')\n",
    "\n",
    "        # check all available characters\n",
    "        char_set = {c for c in vocab if c.isalpha()}\n",
    "        \n",
    "        # candidates dictionary will contain a set of all available tokens to search\n",
    "        candidate_dict = dict().fromkeys(char_set, 0)\n",
    "        \n",
    "        # occurrences will contain all matched tokens and the count, how many times the token has been found.\n",
    "        token_occurrences = OrderedDict()\n",
    "        while len(vocab) < self.vocab_size:\n",
    "            for candidate in candidate_dict.keys():\n",
    "                occurrences = data.count(candidate)\n",
    "                candidate_dict[candidate] = occurrences\n",
    "\n",
    "            candidate_dict = {candidate: count for candidate, count in candidate_dict.items() if count}\n",
    "            vocab.update(set(candidate_dict.keys()))\n",
    "            token_occurrences.update(candidate_dict)\n",
    "\n",
    "            # build new candidates\n",
    "            temp_candidate_set = set()\n",
    "            for char in char_set:\n",
    "                # don't test candidates with occurency <= 2. New candidates won't have occurency higher than 2\n",
    "                temp_candidate_set.update({candidate + char for candidate in candidate_dict.keys() if token_occurrences[candidate] > 2})\n",
    "\n",
    "            candidate_dict = dict().fromkeys(temp_candidate_set, 0)\n",
    "\n",
    "        tokens_to_remove = len(vocab) - self.vocab_size\n",
    "        token_occurrences = OrderedDict(sorted(token_occurrences.items(), key=lambda x: x[1], reverse=True))\n",
    "        for _ in range(tokens_to_remove):\n",
    "            token, _ = token_occurrences.popitem()\n",
    "            vocab.remove(token)\n",
    "\n",
    "        sorted_vocab = self.sort_vocab(vocab)\n",
    "        \n",
    "        # add a special token for unknown tokens\n",
    "        sorted_vocab.append('<unk>')\n",
    "        self.vocab_size += 1 # plus <unk> special token\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def tokenize(self, data, block_size):\n",
    "        for token in self.vocab:\n",
    "            data = data.replace(token, f'#{self.stoi[token]}#')\n",
    "\n",
    "        # If everything went well, first and last characters won't have # pair. Need to trim them\n",
    "        data = data[1:-1]\n",
    "        # Split by ## pairs\n",
    "        tokenized_text = data.split('##')\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if '#' in tokenized:\n",
    "                for unknown_candidate in tokenized.split('#'):\n",
    "                    if unknown_candidate.isnumeric():\n",
    "                        result.append(self.itos[int(unknown_candidate)])\n",
    "                    else:\n",
    "                        result.append('<unk>')\n",
    "            else:\n",
    "                result.append(self.itos[int(tokenized)])\n",
    "\n",
    "        # all texts should have equal size. We can make text length equal by filling text with spaces\n",
    "        for _ in range(block_size - len(result)):\n",
    "            result.append(' ')\n",
    "            \n",
    "        # in case the sentence is longer, than block_size, we trim the sentence\n",
    "        return result[:block_size]\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data):\n",
    "        return ''.join([self.itos[int(i)] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "advised-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
    "tokenizer_modern = Tokenizer(text_modern, vocab_size)\n",
    "tokenizer_original = Tokenizer(text_original, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "significant-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, original, modern, tokenizer_original, tokenizer_modern, block_size):\n",
    "        self.tokenizer_original = tokenizer_original\n",
    "        self.tokenizer_modern = tokenizer_modern\n",
    "        \n",
    "        self.block_size = block_size * 2\n",
    "        self.original = [tokenizer_original.tokenize(t, block_size) for t in original]\n",
    "        self.modern = [tokenizer_modern.tokenize(t, block_size) for t in modern]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The idea is to get a sentence in a modern English\n",
    "        and translate it to Shakespeare English.\n",
    "        \n",
    "        In the init method we already split a sentence into tokens and filled with spaces,\n",
    "        to have an equal sentence size. In this method we just encode the tokens to\n",
    "        ids (a list of numbers), and we're trying to map ids sequences\n",
    "        (original Englisn and modern English)\n",
    "        \"\"\"\n",
    "        \n",
    "        modern_text = self.tokenizer_modern.encode(self.modern[idx])\n",
    "        original_text = self.tokenizer_original.encode(self.original[idx])\n",
    "        dix = modern_text + original_text\n",
    "        \n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        y[:int(self.block_size / 2) - 1] = -100\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "emotional-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle texts by lines\n",
    "texts = list(zip(text_modern.splitlines(), text_original.splitlines()))\n",
    "random.shuffle(texts)\n",
    "\n",
    "text_modern, text_original = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floral-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts into train, test and validation datasets\n",
    "train_dataset_size = round(0.85 * len(text_modern))\n",
    "test_dataset_size = round(0.1 * len(text_modern))\n",
    "valid_dataset_size = round(0.05 * len(text_modern))\n",
    "\n",
    "train_modern = text_modern[:train_dataset_size]\n",
    "test_modern = text_modern[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_modern = text_modern[-valid_dataset_size:]\n",
    "\n",
    "train_original = text_original[:train_dataset_size]\n",
    "test_original = text_original[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_original = text_original[-valid_dataset_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fitted-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 100  # the estimate how long lines the text could be (token count)\n",
    "\n",
    "train_dataset = WordDataset(train_original, train_modern, tokenizer_original, tokenizer_modern, block_size)\n",
    "test_dataset = WordDataset(test_original, test_modern, tokenizer_original, tokenizer_modern, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ruled-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/02/2021 19:22:28 - INFO - mingpt.model -   number of parameters: 1.664922e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(tokenizer_original.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=2, n_head=4, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "anonymous-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-3d89064efa3c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mmingpt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTrainer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTrainerConfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtokens_per_epoch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mblock_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtrain_epochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Projects/DeepWriter/ref/minGPT/mingpt/trainer.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtqdm\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "tokens_per_epoch = len(train_dataset) * block_size\n",
    "train_epochs = 20\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=train_epochs, batch_size=64, learning_rate=3e-4,\n",
    "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "                      num_workers=2)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alright, let's translate some modern English to Shakespeare\n",
    "from mingpt.utils import sample\n",
    "from random import choice\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = choice(range(len(valid_original)))\n",
    "\n",
    "    context = valid_modern[idx]\n",
    "    x = torch.tensor(tokenizer_modern.encode(tokenizer_modern.tokenize(context, block_size)), dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "    predicted = y[block_size:]\n",
    "    completion = tokenizer_original.decode(predicted)\n",
    "    print(f'Modern:             {context}')\n",
    "    print(f'Predicted original: {completion}')\n",
    "    print(f'Real original:      {valid_original[idx]}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-carter",
   "metadata": {},
   "source": [
    "Well, the translation isn't perfect. It's more like a mix of modern and Shakespeare English. To solve it, need more data, a bigger model, use a pre-trained language model, and fine-tune it on Shakespeare texts.\n",
    "\n",
    "Nevertheless, the results above are from the validation dataset, which didn't participate in training and the words are more or less real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}